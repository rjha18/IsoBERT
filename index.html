<!doctype html>
<html class="no-js" lang="">

<head>
  <meta charset="utf-8">
  <title>IsoBERT</title>

  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" href="css/main.css">
</head>

<body>
  <section class="py-5 container">
    <div class="row py-lg-5">
      <div class="col-lg-10 col-md-8 mx-auto text-center">
        <h1 class="fw-light">Nonlinear Dimensionality Reduction on DistilBERT</h1>
      </div>

      <div class="row text-center">
        <div class="col">
          <img src="img/isomap.png" class="figure-img img-fluid rounded" alt="Tenenbaum Isomap">
        </div>
        <h6 class="fw-light">Tenenbaum's Isomap</h6>
      
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Abstract</h3>
        <p class="lead text-muted"><a href="https://arxiv.org/abs/1910.01108">DistilBERT</a> embeddings improve upon their widely used predecessor (<a href="https://arxiv.org/pdf/1910.01108">BERT</a>) by creating similarly performing embeddings that are "smaller, faster, cheaper and lighter". However, at 768 dimensions, the embeddings themselves can be prohibitively large. For this project we investigated the effects of linear (<a href="https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf">Kernel</a> and standard PCA) and non-linear (<a href="https://web.mit.edu/cocosci/Papers/sci_reprint.pdf">Isomap</a>) dimensionality reduction on DistilBERT embeddings, reducing to 128 and 16 dimensions. We find that in many cases, for text classification and sentiment analysis tasks, 16-dimensional, Isomap/PCA reduced DistilBERT embeddings often performed similarly or better when compared to 16 dimensional PCA-reduced embeddings, 128 dimensional PCA-reduced embeddings, and the original embeddings. These results show promise for a novel geometric approach to achieve lower dimensional text embeddings and pave way for data and application specific manifold reductions.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Problem Statement</h3>
        <p class="lead text-muted">State of the art sentence embedding models consist of networks with many layers and millions of parameters, ultimately outputting a vector with hundreds or thousands of features. This leads to a long and expensive process of computing sentence level embeddings and storing large feature vectors. Some sentence embeddings models have looked to solve this problem by reducing the number of layers and parameters in the model to preserve as much information as possible while reducing the computational effort and storage required to produce embeddings. DistilBERT offers comparable performance to other sentence embedding models, outputting the same 768 features, while only using half the number of layers and parameters as the standard BERT model. Other state of the art models like <a href="https://arxiv.org/abs/1909.11942">ALBERT</a> seek to solve this problem in a similar way, while also reducing the dimensionality of the sentence embeddings. While there is a lot of work being done optimizing the models themselves, another approach is reducing the dimensionality of the output embeddings. For many problems, 768 dimension features can be reduced to much smaller feature vectors to accomplish the same goal. Further, we hypothesize that different texts, or different classes of text have different embedded shapes which can be used to reduce the dimensionality of sentence embeddings for text classification and sentiment analysis.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Related Work</h3>

        <h5 class="fw-light">Inspiration</h5>
        <ul class="lead text-muted">
          <li>Application of linear dimensionality reduction for <a href="https://www.aclweb.org/anthology/W19-4328.pdf">word embeddings</a>. Provided justification that dimensionality reduction could work on textual embeddings.</li>
          <li>Library that claims to have similar performance on sentence embeddings with <a href="https://www.sbert.net/examples/training/distillation/README.html">PCA</a>.</li>
          <li>Text dimensionality reduction via <a href="https://towardsdatascience.com/word-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7">Autoencoder</a>.</li>
          <li>Geometric visualization of text embeddings via <a href="https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6">UMAP</a>.</li>
        </ul>

        <h5 class="fw-light">Dimensionality Reduction Resources</h5>
        <ul class="lead text-muted">
          <li>Wikipedia <a href="https://en.wikipedia.org/wiki/Isomap">Isomap</a></li>
          <li>CMU <a href="https://www.cs.cmu.edu/~efros/courses/AP06/presentations/melchior_isomap_demo.pdf">Isomap</a> slides</li>
          <li>SciKit Learn <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold">Manifold Learning</a> Page</li>
        </ul>

        <h5 class="fw-light">BERT and Friends</h5>
        <ul class="lead text-muted">
          <li><a href="https://arxiv.org/pdf/1910.01108">BERT</a></li>
          <li><a href="https://arxiv.org/abs/1909.11942">ALBERT</a></li>
          <li><a href="https://arxiv.org/abs/1910.01108">DistilBERT</a></li>
        </ul>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Motivation</h3>
        <p class="lead text-muted">Text has latent structure and shape. For example, the structure and syntax of a recipe is noticeably different than that of an academic paper or poetry perhaps. So, the question for trying to reduce the dimensionality of DistilBERT then is how well the embeddings encapsulate an internal geometry of text and how can we best maintain this geometry. To answer this, we reduced the DistilBERT embeddings of the only 30 unique lines in Miley Cyrus’ “Party in the U.S.A” and a quasi-random selection of 30 sentences from Microsoft’s Security Agreement below. We start by reducing the embeddings to 64 dimensions by PCA and then to 3 using PCA again (on the right) and Isomap (on the left). As you can see on the right, DistilBERT embeddings do, in fact, have some interesting shape: the Miley Cyrus lyrics were largely mapped to a single dimension whereas the security agreement was sparse. On the left, however, the distinction is less clear.</p>
        <div class="row text-center">
          <div class="col">
            <img src="img/mm_iso.png" class="figure-img img-fluid rounded" alt="Isomap Miley and Microsoft">
          </div>
          <div class="col">
            <img src="img/mm_small.png" class="figure-img img-fluid rounded" alt="Small PCA Miley and Microsoft">
          </div>
        </div>
        <p class="lead text-muted">Now, you may ask yourself why not reduce the dimensionality via Isomap directly from 768 to 3. Notably, this takes a (prohibitively) long time, and in our experimentation much of the important dimensionality is maintained with a linear reduction from 768 to 64 dimensions.</p>
        <p class="lead text-muted">Separately, Isomap is not the only non-linear reduction technique, but we found <a href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">T-SNE</a> to be far too slow and the entire <a href="https://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf">LLE</a> family struggled to maintain interesting information. We hypothesize a couple of reasons for this:</p>
        <ol class="lead text-muted">
          <li>We use only a small number of datapoints tested for each dataset (5000-15000). LLE is known to be a weak approach for non-uniform sample densities and the lack of data could have led to some roughness.</li>
          <li>Our dimensionality reduction techniques were agnostic to class. Since LLE relies on local neighborhoods, a neighborhood with multiple classes could be reduced to be disproportionately similar in a lower dimension. However, Isomap however reduces on global geodesic distance. This means that the pairwise geodesic distance is largely maintained and the elements of the same class should stay closer.</li>
        </ol>
        <p class="lead text-muted">In future work we would try a supervised version of LLE and other Isomap methods (Conformal and Landmark). In addition, we tried Kernel PCA, and found it be much slower with similar results to standard PCA. This, however, should be studied more carefully.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Methodology</h3>
        <h5 class="fw-light">Embeddings:</h5>
        <p class="lead text-muted">From the DistilBERT embeddings we generated three embeddings:</p>
        <ol class="lead text-muted">
          <li>(Large) PCA</li>
          <li>(Small) PCA</li>
          <li>IsoBERT</li>
        </ol>
        <p class="lead text-muted">The first, Large PCA was generated by performing a 64-dimensional PCA reduction on the DistilBERT embeddings for a given dataset. From Large PCA, we performed another PCA (16 dimensional this time) to create Small PCA and 16 dimensional Isomap to create our third embedding we dub ‘IsoBERT’. We tested each of these embeddings with a standard fine-tuning network described in the Experiments/Evaluation section.</p>
        <div class="row text-center">
          <div class="col">
            <img src="img/embeddings.png" class="figure-img img-fluid rounded img-limit" alt="Embeddings">
          </div>
        </div>
        <h5 class="fw-light">Datasets:</h5>
        <p class="lead text-muted">We tested on four datasets, three of which were text classification and the last being sentiment analysis. Notably, for the fake news dataset and the AGNews dataset, for computational resource reasons, we only analyze the titles of the articles. Details Below:</p>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Datasets</th>
              <th scope="col">Classes</th>
              <th scope="col">Task</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Amazon Reviews</th>
              <td>2</td>
              <td>Sentiment Analysis</td>
            </tr>
            <tr>
              <th scope="row">AG_NEWS</th>
              <td>5</td>
              <td>Text Classification</td>
            </tr>
            <tr>
              <th scope="row">SMS_SPAM</th>
              <td>2</td>
              <td>Text Classification </td>
            </tr>
            <tr>
              <th scope="row">Fake News</th>
              <td>2</td>
              <td>Text Classification</td>
            </tr>
          </tbody>
        </table>

      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Experiments</h3>
        <p class="lead text-muted">In order to test our hypotheses, we tested a variety of different hyperparameters for our PCA and Isomap algorithms to find the optimal number of components and neighbors for each algorithm. We discovered that when initially reducing the dimensionality with PCA, we could capture 80% of the variance with only the top 64 components, which contained enough information for consistently strong performance among all datasets. As discussed in the methodology section, we experimented with a variety of different manifold learning techniques, but we found the best results (both runtime, memory and performance) to be Isomap. To further reduce the dimensionality of the embeddings, we empirically found running Isomap with 32 neighbors and reducing the embeddings down to a final embedding of 16 dimensions. We sought to reduce the dimensions as low as possible in both the PCA and Isomap stages without compromising downstream accuracy. Although our experimentation at this stage was not fully exhaustive, these parameters seemed to produce strong enough intermediate results to move forward to our evaluation.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Evaluation</h3>
        <p class="lead text-muted">To maintain consistency in our evaluation, we trained all of our models using the same DistilBERT embeddings on the datasets. All datasets were shuffled, shortened to the top 5000 rows, then finally split into 75% training and 25% testing datasets. We trained a simple 2-layer (32 hidden dim) logistic regression model directly on the sentence embeddings, tuning the learning rate to maximize performance. This served as our baseline for the other three models we evaluated. We evaluated the three reduced dimension models on the same embeddings, starting the learning rate at 0.001 and training, with a learning rate adjustment of 0.1 when the train loss plateaued until each model appeared to converge, reporting training loss and accuracy on the test set. We repeated this procedure on each dataset with the exact same conditions and hyperparameters. </p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Results</h3>
        <p class="lead text-muted">On the four benchmark datasets we selected, all three forms of dimensionality reduction we chose performed well. </p>
        <p class="text-muted">Big PCA (n_components=64) performed extremely well in all four tests. Compared to logistic regression, Big PCA achieved better performance on two datasets (AG_NEWS, SMS_SPAM) and was within 1.3% test accuracy on the other two datasets. </p>
        <p class="text-muted">Isomap (n_components=16) performed quite similarly to Big PCA on most datasets. We achieved superior test accuracy to logistic regression on the Amazon Review dataset, and were within 2.2% test accuracy on AG_NEWS and SMS_SPAM. The only shortcoming was a difference of 5% test accuracy on the Fake News dataset.</p>
        <p class="text-muted">Small PCA (n_components=16) unsurprisingly maintained performance slightly worse than Big PCA across all four datasets. Still, Small PCA achieved test accuracy within 4.3% on three datasets (AG_NEWS, SMS_SPAM, Amazon Reviews), and performed only 5.4% worse on the fake news dataset.</p>

        <a style="text-decoration: none;";" href="https://www.kaggle.com/bittlingmayer/amazonreviews"><p class="lead text-muted">Amazon Reviews (2 Classes, sentiment analysis, trained on reviews)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.4735505581</td>
              <td>80.48000336</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.5291960239</td>
              <td>79.19999695</td>
            </tr>
            <tr>
              <th scope="row">Isomap</th>
              <td>0.3619812727</td>
              <td>80.87999725 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.4090490937</td>
              <td>77.51999664 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv"><p class="lead text-muted">AG_NEWS (5 Classes, text classification, trained on headlines only)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.3054822385</td>
              <td>80.63999939</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.4542485178</td>
              <td>80.80000305</td>
            </tr>
            <tr>
              <th scope="row">Isomap</th>
              <td>0.7634957433</td>
              <td>78.48000336 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.5140572786</td>
              <td>76.31999969 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/"><p class="lead text-muted">SMS_SPAM (2 Classes, text classification, trained on text message)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.008057968691</td>
              <td>98.19999695</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.01945737004</td>
              <td>99</td>
            </tr>
            <tr>
              <th scope="row">Isomap</th>
              <td>0.01761851087</td>
              <td>97.80000305 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.02542848885</td>
              <td>98.59999847 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="https://www.kaggle.com/c/fake-news/data"><p class="lead text-muted">Fake News (2 Classes, text classification, trained on headlines only)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.04637527466</td>
              <td>93</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.1332252473</td>
              <td>91.80000305</td>
            </tr>
            <tr>
              <th scope="row">Isomap</th>
              <td>0.2197600007</td>
              <td>86 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.2089808136</td>
              <td>87.59999847 </td>
            </tr>
          </tbody>
        </table>
        <a href="https://docs.google.com/spreadsheets/d/1lRPDd_auOxFBoutjD4mF5BQsYw7ton4h7iod7lLl3D0/edit?usp=sharing"><p class="text-muted">Full Results</p></a>
        <p class="lead text-muted">Selected Embedding Visualizations</p>
        <p class="text-muted">Reduced dimension DistilBERT embeddings visualized in 3 dimensions by plotting the top three components of the embedding.</p>
        <div class="row text-center">
          <h5>SMS_SPAM (Isomap, Big PCA)</h5>
          <div class="col">
            <img src="img/SMS_SPAM (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>
          <div class="col">
            <img src="img/SMS_SPAM (Big PCA) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>

          <h5>AG_NEWS (Isomap, Big PCA)</h5>
          <div class="col">
            <img src="img/AG_NEWS (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>
          <div class="col">
            <img src="img/AG_NEWS (Big PCA) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>

          <h5>Amazon Reviews (Isomap, Big PCA)</h5>
          <div class="col">
            <img src="img/Amazon Review (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>
          <div class="col">
            <img src="img/Amazon Review (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>

          <h5>Fake News (Isomap, Big PCA)</h5>
          <div class="col">
            <img src="img/Fake News (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>
          <div class="col">
            <img src="img/Fake News (Big PCA) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          </div>
        </div>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Examples</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Video</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>
    </div>
  </section>
  <section class="text-left container">
    <div class="row py-lg-5">
      
    </div>
  </section>
</body>

</html>

