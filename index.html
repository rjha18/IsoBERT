<!doctype html>
<html class="no-js" lang="">

<head>
  <meta charset="utf-8">
  <title>IsoBERT</title>

  <link rel="stylesheet" href="css/bootstrap.min.css">
</head>

<body>
  <section class="py-5 container">
    <div class="row py-lg-5">
      <div class="col-lg-10 col-md-8 mx-auto text-center">
        <h1 class="fw-light">Nonlinear Dimensionality Reduction on BERT</h1>
      </div>
      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Abstract</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
        <p class="text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Problem Statement</h3>
        <p class="lead text-muted">State of the art sentence embedding models consist of networks with many layers and millions of parameters, ultimately outputting a vector with hundreds or thousands of features. This leads to a long and expensive process of computing sentence level embeddings and storing large feature vectors. Some sentence embeddings models have looked to solve this problem by reducing the number of layers and parameters in the model to preserve as much information as possible while reducing the computational effort and storage required to produce embeddings. DistilBERT offers comparable performance to other sentence embedding models, outputting the same 768 features, while only using half the number of layers and parameters as the standard BERT model (https://arxiv.org/pdf/1910.01108.pdf). Other state of the art models like ALBERT (https://arxiv.org/abs/1909.11942) seek to solve this problem in a similar way, while also reducing the dimensionality of the sentence embeddings. While there is a lot of work being done optimizing the models themselves, another approach is reducing the dimensionality of the output embeddings. For many problems, 768 dimension features can be reduced to much smaller feature vectors to accomplish the same goal. Further, we hypothesize that different texts, or different classes of text have different embedded shapes which can be used to reduce the dimensionality of sentence embeddings for text classification and sentiment analysis.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Related Work</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Methadology</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Experiments/Evaluation</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Results</h3>
        <p class="lead text-muted">On the four benchmark datasets we selected, all three forms of dimensionality reduction we chose performed well. </p>
        <p class="text-muted">Big PCA (n_components=64) performed extremely well in all four tests. Compared to logistic regression, Big PCA achieved better performance on two datasets (AG_NEWS, SMS_SPAM) and was within 1.3% test accuracy on the other two datasets. </p>
        <p class="text-muted">Isomap (n_components=16) performed quite similarly to Big PCA on most datasets. We achieved superior test accuracy to logistic regression on the Amazon Review dataset, and were within 2.2% test accuracy on AG_NEWS and SMS_SPAM. The only shortcoming was a difference of 5% test accuracy on the Fake News dataset.</p>
        <p class="text-muted">Small PCA (n_components=16) unsurprisingly maintained performance slightly worse than Big PCA across all four datasets. Still, Small PCA achieved test accuracy within 4.3% on three datasets (AG_NEWS, SMS_SPAM, Amazon Reviews), and performed only 5.4% worse on the fake news dataset.</p>




        <a style="text-decoration: none;";" href="https://www.kaggle.com/bittlingmayer/amazonreviews"><p class="lead text-muted">Amazon Reviews (2 Classes, sentiment analysis, trained on reviews)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.4735505581</td>
              <td>80.48000336</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.5291960239</td>
              <td>79.19999695</td>
            </tr>
            <tr>
              <th scope="row">IsoMAP</th>
              <td>0.3619812727</td>
              <td>80.87999725 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.4090490937</td>
              <td>77.51999664 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv"><p class="lead text-muted">AG_NEWS (5 Classes, text classification, trained on headlines only)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.3054822385</td>
              <td>80.63999939</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.4542485178</td>
              <td>80.80000305</td>
            </tr>
            <tr>
              <th scope="row">IsoMAP</th>
              <td>0.7634957433</td>
              <td>78.48000336 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.5140572786</td>
              <td>76.31999969 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/"><p class="lead text-muted">SMS_SPAM (2 Classes, text classification, trained on text message)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.008057968691</td>
              <td>98.19999695</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.01945737004</td>
              <td>99</td>
            </tr>
            <tr>
              <th scope="row">IsoMAP</th>
              <td>0.01761851087</td>
              <td>97.80000305 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.02542848885</td>
              <td>98.59999847 </td>
            </tr>
          </tbody>
        </table>

        <a style="text-decoration: none;";" href="https://www.kaggle.com/c/fake-news/data"><p class="lead text-muted">Fake News (2 Classes, text classification, trained on headlines)</p></a>
        <table class="table">
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Train Loss</th>
              <th scope="col">Test Accuracy (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope="row">Logistic Regression</th>
              <td>0.04637527466</td>
              <td>93</td>
            </tr>
            <tr>
              <th scope="row">Big PCA</th>
              <td>0.1332252473</td>
              <td>91.80000305</td>
            </tr>
            <tr>
              <th scope="row">IsoMAP</th>
              <td>0.2197600007</td>
              <td>86 </td>
            </tr>
            <tr>
              <th scope="row">Small PCA</th>
              <td>0.2089808136</td>
              <td>87.59999847 </td>
            </tr>
          </tbody>
        </table>
        <p class="lead text-muted">Selected Embedding Visualizations</p>
        <p class="text-muted">Reduced dimension DistilBERT embeddings visualized in 3 dimensions by plotting the top three components of the embedding.</p>
        <figure class="figure">
          <img src="img/SMS_SPAM (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="SMS_SPAM Isomap Embedding Visualization">
          <figcaption class="figure-caption">SMS_SPAM Isomap Embedding Visualization</figcaption>

          <img src="img/AG_NEWS (Big PCA) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="AG_NEWS Big PCA Embedding Visualization">
          <figcaption class="figure-caption">AG_NEWS Big PCA Embedding Visualization</figcaption>
          
          <img src="img/Fake News (Small PCA) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="Fake News Small PCA Embedding Visualization">
          <figcaption class="figure-caption">Fake News Small PCA Embedding Visualization</figcaption>

          <img src="img/Amazon Review (Isomap) - Embedding Graph.png" class="figure-img img-fluid rounded" alt="Amazon Review Isomap Embedding Visualization">
          <figcaption class="figure-caption">Amazon Review Isomap Embedding Visualization </figcaption>
        </figure>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Examples</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>

      <div class="py-3 col-lg-10 col-md-0 mx-auto">
        <h3 class="fw-light">Video</h3>
        <p class="lead text-muted">Something short and leading about the collection below—its contents, the creator, etc. Make it short and sweet, but not too short so folks don’t simply skip over it entirely.</p>
      </div>
    </div>
  </section>
  <section class="text-left container">
    <div class="row py-lg-5">
      
    </div>
  </section>
</body>

</html>

